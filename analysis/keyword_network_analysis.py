import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from collections import Counter
from itertools import combinations
import re
from pathlib import Path

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


def extract_keyword_pairs(text, stopwords, min_length=2):
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ìŒ ì¶”ì¶œ"""
    if pd.isna(text):
        return []

    text = str(text)

    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text)

    # í† í°í™”
    tokens = text.split()

    # í•„í„°ë§
    keywords = [
        w for w in tokens
        if len(w) >= min_length
           and w not in stopwords
           and not w.isdigit()
    ]

    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
    seen = set()
    unique_keywords = []
    for k in keywords:
        if k not in seen:
            seen.add(k)
            unique_keywords.append(k)

    # í‚¤ì›Œë“œ ìŒ ìƒì„± (ê°™ì€ ëŒ“ê¸€ ë‚´ì—ì„œ í•¨ê»˜ ë“±ì¥)
    if len(unique_keywords) >= 2:
        return list(combinations(unique_keywords, 2))
    return []


def create_network_graph(pairs, top_n=30, min_weight=2):
    """ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±"""
    # ìŒì˜ ë¹ˆë„ ê³„ì‚°
    pair_counts = Counter(pairs)

    # ìµœì†Œ ë¹ˆë„ í•„í„°ë§
    filtered_pairs = {pair: count for pair, count in pair_counts.items() if count >= min_weight}

    if not filtered_pairs:
        print("âš ï¸  ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œ ìŒì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    # ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()

    for (word1, word2), weight in filtered_pairs.items():
        G.add_edge(word1, word2, weight=weight)

    # ì—°ê²°ì„±ì´ ë†’ì€ ë…¸ë“œë§Œ ì„ íƒ
    if len(G.nodes()) > top_n:
        # ì°¨ìˆ˜(degree)ê°€ ë†’ì€ ë…¸ë“œ ì„ íƒ
        degrees = dict(G.degree())
        top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:top_n]
        top_node_names = [node for node, _ in top_nodes]
        G = G.subgraph(top_node_names).copy()

    return G


def visualize_network(G, platform_name, output_path):
    """ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”"""
    if G is None or len(G.nodes()) == 0:
        print(f"âš ï¸  {platform_name}: ì‹œê°í™”í•  ë„¤íŠ¸ì›Œí¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    plt.figure(figsize=(16, 12))

    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)

    # ë…¸ë“œ í¬ê¸° (ì—°ê²° ìˆ˜ì— ë¹„ë¡€)
    degrees = dict(G.degree())
    node_sizes = [degrees[node] * 300 for node in G.nodes()]

    # ì—£ì§€ ë‘ê»˜ (ê°€ì¤‘ì¹˜ì— ë¹„ë¡€)
    edges = G.edges()
    weights = [G[u][v]['weight'] for u, v in edges]
    max_weight = max(weights) if weights else 1
    edge_widths = [w / max_weight * 5 for w in weights]

    # ë…¸ë“œ ê·¸ë¦¬ê¸°
    nx.draw_networkx_nodes(
        G, pos,
        node_size=node_sizes,
        node_color='lightblue',
        alpha=0.7,
        edgecolors='navy',
        linewidths=2
    )

    # ì—£ì§€ ê·¸ë¦¬ê¸°
    nx.draw_networkx_edges(
        G, pos,
        width=edge_widths,
        alpha=0.3,
        edge_color='gray'
    )

    # ë¼ë²¨ ê·¸ë¦¬ê¸°
    nx.draw_networkx_labels(
        G, pos,
        font_size=10,
        font_family='Malgun Gothic',
        font_weight='bold'
    )

    plt.title(f'{platform_name} í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬\n(í•¨ê»˜ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ)',
              fontsize=16, fontweight='bold', pad=20)
    plt.axis('off')
    plt.tight_layout()

    # ì €ì¥
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"   âœ“ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ì €ì¥: {output_path}")
    plt.close()


def analyze_network_statistics(G, platform_name):
    """ë„¤íŠ¸ì›Œí¬ í†µê³„ ë¶„ì„"""
    if G is None or len(G.nodes()) == 0:
        return

    print(f"\nğŸ“Š {platform_name} ë„¤íŠ¸ì›Œí¬ í†µê³„:")
    print(f"   ë…¸ë“œ ìˆ˜: {G.number_of_nodes()}ê°œ")
    print(f"   ì—£ì§€ ìˆ˜: {G.number_of_edges()}ê°œ")

    # ì¤‘ì‹¬ì„±ì´ ë†’ì€ í‚¤ì›Œë“œ (ê°€ì¥ ë§ì´ ì—°ê²°ëœ)
    degrees = dict(G.degree())
    top_central = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:5]

    print(f"\n   ğŸ”— ê°€ì¥ ë§ì´ ì—°ê²°ëœ í‚¤ì›Œë“œ:")
    for word, degree in top_central:
        print(f"      {word}: {degree}ê°œ ì—°ê²°")

    # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ì—°ê²° (ìì£¼ í•¨ê»˜ ë“±ì¥)
    edges_with_weights = [(u, v, G[u][v]['weight']) for u, v in G.edges()]
    top_pairs = sorted(edges_with_weights, key=lambda x: x[2], reverse=True)[:5]

    print(f"\n   ğŸ’¬ ìì£¼ í•¨ê»˜ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ ìŒ:")
    for word1, word2, weight in top_pairs:
        print(f"      {word1} â†” {word2}: {weight}íšŒ")


def keyword_network_main():
    """í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹œì‘")
    print("=" * 60)

    # ë¶ˆìš©ì–´ ì„¤ì •
    stopwords = {
        "ì´ê±°", "ê·¸ëƒ¥", "ì§„ì§œ", "ì •ë§", "ë„ˆë¬´",
        "ì‚¬ëŒ", "ì˜ìƒ", "ë‰´ìŠ¤", "ê¸°ì‚¬", "ëŒ“ê¸€",
        "ê²ƒ", "ìˆ˜", "ì´ê²Œ", "ì €ê²Œ", "ìš”ê±°", "ê·¸ê±°",
        "ì´ê±´", "ì €ê±´", "ìˆë‹¤", "ì—†ë‹¤", "í•˜ë‹¤", "ë˜ë‹¤",
        "ì´ë‹¤", "ì•„ë‹ˆë‹¤", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ°ë°", "í•˜ì§€ë§Œ",
        "ê·¸ë˜ì„œ", "ì™œëƒí•˜ë©´", "ì´ë ‡ê²Œ", "ì €ë ‡ê²Œ", "ì–´ë–»ê²Œ",
        "ë­”ê°€", "ì•½ê°„", "ì¢€", "ë”", "ì•ˆ", "ëª»", "ë‹¤",
        "ë˜", "ë°", "ë“±", "ìˆëŠ”", "ì—†ëŠ”", "í•˜ëŠ”", "ë˜ëŠ”",
        "ì´ëŸ°", "ì €ëŸ°", "ã…‹ã…‹", "ã…ã…", "ã… ã… ", "ã…œã…œ"
    }

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path("../data/processed")
    output_dir.mkdir(parents=True, exist_ok=True)

    # ë„¤ì´ë²„ ëŒ“ê¸€ ë¶„ì„
    naver_path = Path("../data/naver_comments_20260102_121759.csv")
    if naver_path.exists():
        print("\nğŸ” ë„¤ì´ë²„ ëŒ“ê¸€ ë¶„ì„ ì¤‘...")
        df_naver = pd.read_csv(naver_path)

        # ëŒ“ê¸€ ì»¬ëŸ¼ ì°¾ê¸°
        text_col = None
        for col in ['ëŒ“ê¸€', 'comment', 'content', 'text']:
            if col in df_naver.columns:
                text_col = col
                break

        if text_col:
            # í‚¤ì›Œë“œ ìŒ ì¶”ì¶œ
            all_pairs = []
            for text in df_naver[text_col]:
                pairs = extract_keyword_pairs(text, stopwords)
                all_pairs.extend(pairs)

            print(f"   ì¶”ì¶œëœ í‚¤ì›Œë“œ ìŒ: {len(all_pairs)}ê°œ")

            # ë„¤íŠ¸ì›Œí¬ ìƒì„± ë° ì‹œê°í™”
            G_naver = create_network_graph(all_pairs, top_n=30, min_weight=2)
            if G_naver:
                visualize_network(
                    G_naver,
                    "ë„¤ì´ë²„",
                    output_dir / "network_naver.png"
                )
                analyze_network_statistics(G_naver, "ë„¤ì´ë²„")

                # ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ì €ì¥
                edge_data = [(u, v, G_naver[u][v]['weight'])
                             for u, v in G_naver.edges()]
                edge_df = pd.DataFrame(edge_data,
                                       columns=['keyword1', 'keyword2', 'weight'])
                edge_df['platform'] = 'naver'
                edge_df.to_csv(
                    output_dir / "network_edges_naver.csv",
                    index=False,
                    encoding='utf-8-sig'
                )

    # ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„
    youtube_path = Path("../data/utube_comments_20260102_121947.csv")
    if youtube_path.exists():
        print("\nğŸ” ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì¤‘...")
        df_youtube = pd.read_csv(youtube_path)

        # ëŒ“ê¸€ ì»¬ëŸ¼ ì°¾ê¸°
        text_col = None
        for col in ['ëŒ“ê¸€', 'comment', 'content', 'text']:
            if col in df_youtube.columns:
                text_col = col
                break

        if text_col:
            # í‚¤ì›Œë“œ ìŒ ì¶”ì¶œ
            all_pairs = []
            for text in df_youtube[text_col]:
                pairs = extract_keyword_pairs(text, stopwords)
                all_pairs.extend(pairs)

            print(f"   ì¶”ì¶œëœ í‚¤ì›Œë“œ ìŒ: {len(all_pairs)}ê°œ")

            # ë„¤íŠ¸ì›Œí¬ ìƒì„± ë° ì‹œê°í™”
            G_youtube = create_network_graph(all_pairs, top_n=30, min_weight=2)
            if G_youtube:
                visualize_network(
                    G_youtube,
                    "ìœ íŠœë¸Œ",
                    output_dir / "network_youtube.png"
                )
                analyze_network_statistics(G_youtube, "ìœ íŠœë¸Œ")

                # ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ì €ì¥
                edge_data = [(u, v, G_youtube[u][v]['weight'])
                             for u, v in G_youtube.edges()]
                edge_df = pd.DataFrame(edge_data,
                                       columns=['keyword1', 'keyword2', 'weight'])
                edge_df['platform'] = 'youtube'
                edge_df.to_csv(
                    output_dir / "network_edges_youtube.csv",
                    index=False,
                    encoding='utf-8-sig'
                )

    print("\n" + "=" * 60)
    print("âœ“ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")


if __name__ == "__main__":
    keyword_network_main()