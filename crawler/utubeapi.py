import os
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class YouTubeCommentCrawler:
    def __init__(self, api_key):
        """
        YouTube Data API v3ë¥¼ ì‚¬ìš©í•œ ëŒ“ê¸€ í¬ë¡¤ëŸ¬

        Args:
            api_key: YouTube Data API í‚¤
        """
        self.youtube = build('youtube', 'v3', developerKey=api_key)

    def extract_video_id(self, url):
        """
        YouTube URLì—ì„œ video ID ì¶”ì¶œ

        Args:
            url: YouTube ë¹„ë””ì˜¤ URL
        Returns:
            video_id: ì¶”ì¶œëœ ë¹„ë””ì˜¤ ID
        """
        if 'v=' in url:
            return url.split('v=')[1].split('&')[0]
        elif 'youtu.be/' in url:
            return url.split('youtu.be/')[1].split('?')[0]
        else:
            return url

    def get_comments(self, video_id, max_results=5000):
        """
        íŠ¹ì • ë¹„ë””ì˜¤ì˜ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° (ëŒ€ëŒ“ê¸€ ì œì™¸)

        Args:
            video_id: YouTube ë¹„ë””ì˜¤ ID
            max_results: ê°€ì ¸ì˜¬ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜
        Returns:
            comments: ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸
        """
        comments = []

        try:
            request = self.youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=min(100, max_results),
                order="relevance"  # 'time' ë˜ëŠ” 'relevance'
            )

            while request and len(comments) < max_results:
                response = request.execute()

                for item in response['items']:
                    # ìµœìƒìœ„ ëŒ“ê¸€ë§Œ ìˆ˜ì§‘
                    comment = item['snippet']['topLevelComment']['snippet']
                    comments.append({
                        'ëŒ“ê¸€': comment['textDisplay'],
                        'ì¢‹ì•„ìš”': comment['likeCount']
                    })

                    if len(comments) >= max_results:
                        break

                # ë‹¤ìŒ í˜ì´ì§€
                if 'nextPageToken' in response and len(comments) < max_results:
                    request = self.youtube.commentThreads().list(
                        part="snippet",
                        videoId=video_id,
                        maxResults=min(100, max_results - len(comments)),
                        pageToken=response['nextPageToken'],
                        order="relevance"
                    )
                else:
                    break

        except HttpError as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            if e.resp.status == 403:
                print("ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆê±°ë‚˜ API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")

        return comments[:max_results]

    def save_to_csv(self, comments, filename=None, save_dir="data"):
        """
        ëŒ“ê¸€ì„ CSV íŒŒì¼ë¡œ ì €ì¥

        Args:
            comments: ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸
            filename: ì €ì¥í•  íŒŒì¼ëª…
            save_dir: ì €ì¥í•  í´ë” ê²½ë¡œ (ê¸°ë³¸ê°’: data)
        """
        # ì €ì¥ í´ë” ìƒì„± (ì—†ìœ¼ë©´)
        os.makedirs(save_dir, exist_ok=True)
        
        if not filename:
            filename = f"youtube_comments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        # ì „ì²´ ê²½ë¡œ ìƒì„±
        filepath = os.path.join(save_dir, filename)
        
        df = pd.DataFrame(comments)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"'{filepath}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(comments)}ê°œ ëŒ“ê¸€)")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
    API_KEY = os.getenv('YOUTUBE_API_KEY')
    
    if not API_KEY:
        print("âŒ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("ğŸ“ .env íŒŒì¼ì— YOUTUBE_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        exit(1)

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = YouTubeCommentCrawler(API_KEY)

    # YouTube URL ë˜ëŠ” ë¹„ë””ì˜¤ ID
    video_url = input('youtube_urlì„ ì…ë ¥í•˜ì‹œì˜¤: ')
    video_id = crawler.extract_video_id(video_url)

    # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° (ì›í•˜ëŠ” ê°œìˆ˜ë¡œ ë³€ê²½ ê°€ëŠ¥)
    print(f"ë¹„ë””ì˜¤ ID: {video_id}ì˜ ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    comments = crawler.get_comments(video_id, max_results=5000)  # 5000ê°œë¡œ ë³€ê²½

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nì´ {len(comments)}ê°œì˜ ëŒ“ê¸€ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.\n")
    for i, comment in enumerate(comments[:5], 1):
        print(f"{i}. {comment['ëŒ“ê¸€'][:50]}... (ì¢‹ì•„ìš”: {comment['ì¢‹ì•„ìš”']})\n")

    # CSVë¡œ ì €ì¥
    crawler.save_to_csv(comments, save_dir="data/utube")